{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.compat import v1 as tf1\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import pandas as pd\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "from imp import reload\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import correlated_topic_model as ctmd\n",
    "import dynamic_correlated_topic_model as dctm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTM SotU\n",
    "\n",
    "Download from https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017. Extract in a folder, and then run the following on the `sotu` folder which was inside the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data/sotu'\n",
    "path = '/Users/federicot/Downloads/1660_131107_bundle_archive/sotu/'\n",
    "\n",
    "import datasets\n",
    "df, corpus, vocabulary = datasets.get_sotu(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "scaler = sklearn.preprocessing.MinMaxScaler([-1, 1])\n",
    "index_points = scaler.fit_transform(df.years[:, None])\n",
    "# index_points = year.astype(np.float64)[:, None]\n",
    "\n",
    "# index_points = df.years.values.astype(np.float64)[:, None]\n",
    "# inducing_index_points = np.unique(index_points)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(corpus.todense().astype(np.float64), -2)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "(X_tr, X_ts, index_tr, index_ts, X_tr_sorted, X_ts_sorted,\n",
    " index_tr_sorted, index_ts_sorted\n",
    ") = datasets.train_test_split(X, index_points)\n",
    "\n",
    "inverse_transform_fn = lambda x: pd.to_datetime(scaler.inverse_transform(x)[:, 0], format='%Y')\n",
    "df_train = pd.DataFrame(X_tr_sorted[:, 0, :])\n",
    "df_train['years'] = inverse_transform_fn(index_tr_sorted)\n",
    "\n",
    "df_test = pd.DataFrame(X_ts_sorted[:, 0, :])\n",
    "df_test['years'] = inverse_transform_fn(index_ts_sorted)\n",
    "\n",
    "print(\"Dataset shape: tr: {}, ts: {}\".format(X_tr.shape, X_ts.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the data before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse as sp\n",
    "# dok_tr = sp.dok_matrix(X_tr_sorted[:, 0, :])\n",
    "# dok_ts = sp.dok_matrix(X_ts_sorted[:, 0, :])\n",
    "\n",
    "# name = 'sotu'\n",
    "# save_pickle(dok_tr, '{}_tr_doc.pkl'.format(name))\n",
    "# save_pickle(dok_ts, '{}_ts_doc.pkl'.format(name))\n",
    "# save_pickle(vocabulary, '{}_vocabulary.pkl'.format(name))\n",
    "\n",
    "# save_pickle(index_tr, '{}_tr_index.pkl'.format(name))\n",
    "# save_pickle(index_ts, '{}_ts_index.pkl'.format(name))\n",
    "\n",
    "# X_sorted = np.vstack((X_tr_sorted[:, 0, :], X_ts_sorted[:, 0, :]))\n",
    "# print_to_file_for_gdtm(\n",
    "#     df_train.append(df_test),\n",
    "#     vocabulary,\n",
    "#     sp.dok_matrix(X_sorted),\n",
    "#     filename='sotu_all',\n",
    "#     path='../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "total_samples = X_tr.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.zip(\n",
    "    tuple(map(tf.data.Dataset.from_tensor_slices,\n",
    "        (X_tr, index_tr))))\n",
    "dataset = dataset.shuffle(total_samples, reshuffle_each_iteration=True)\n",
    "data_tr = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_index_points_beta = np.linspace(-1, 1, 6)[:, None]\n",
    "inducing_index_points_mu = np.linspace(-1, 1, 6)[:, None]\n",
    "inducing_index_points_ell = np.linspace(-1, 1, 6)[:, None]\n",
    "\n",
    "dtype = np.float64\n",
    "amplitude_beta = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_beta')\n",
    "length_scale_beta = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_beta')\n",
    "kernel_beta = tfk.MaternOneHalf(amplitude=amplitude_beta, length_scale=length_scale_beta)\n",
    "\n",
    "amplitude_mu = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name=\"amplitude_mu\")\n",
    "length_scale_mu = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name=\"length_scale_mu\")\n",
    "kernel_mu = tfk.ExponentiatedQuadratic(amplitude=amplitude_mu, length_scale=length_scale_mu)\n",
    "\n",
    "amplitude_ell = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_ell')\n",
    "length_scale_ell = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_ell')\n",
    "kernel_ell = tfk.ExponentiatedQuadratic(amplitude=amplitude_ell, length_scale=length_scale_ell)\n",
    "\n",
    "reload(ctmd)\n",
    "reload(dctm);\n",
    "\n",
    "losses = []\n",
    "perplexities = []\n",
    "\n",
    "mdl = dctm.DCTM(\n",
    "    n_topics=20, n_words=vocabulary.size,\n",
    "    kernel_beta=kernel_beta,\n",
    "    index_points_beta=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_beta=inducing_index_points_beta,\n",
    "    kernel_ell=kernel_ell,\n",
    "    kernel_mu=kernel_mu,\n",
    "    index_points_mu=np.unique(index_tr)[:, None],\n",
    "    index_points_ell=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_mu=inducing_index_points_mu,\n",
    "    inducing_index_points_ell=inducing_index_points_ell,\n",
    "    layer_sizes=(500, 300, 200),\n",
    "    jitter_beta=1e-6,\n",
    "    jitter_mu=1e-5, \n",
    "    jitter_ell=1e-6,\n",
    "    encoder_jitter=1e-8,dtype=dtype)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "# optimizer.iterations = tf1.train.get_or_create_global_step()\n",
    "\n",
    "# import os\n",
    "# checkpoint_directory = \"../tmp/training_checkpoints-30-topics\"\n",
    "# checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-sou-20t\")\n",
    "# checkpoint = tf.train.Checkpoint(model=mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 2 # 1000\n",
    "pbar = tqdm(range(n_iter), disable=False)\n",
    "with tf.device('gpu'): \n",
    "    for epoch in pbar:\n",
    "        loss_value = 0\n",
    "        perplexity_value = 0\n",
    "\n",
    "        for x_batch, index_points_batch in data_tr:\n",
    "            loss, perpl = mdl.batch_optimize(\n",
    "                x_batch,\n",
    "                optimizer=optimizer,\n",
    "                observation_index_points=index_points_batch,\n",
    "                trainable_variables=None,\n",
    "                kl_weight=float(x_batch.shape[0]) / float(total_samples))\n",
    "            loss = tf.reduce_mean(loss, 0)\n",
    "            loss_value += loss\n",
    "            perplexity_value += perpl\n",
    "        pbar.set_description(\n",
    "        'loss {:.3e}, perpl {:.3e}'.format(loss_value, perplexity_value))\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        perplexities.append(perplexity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, perpl = mdl.loss_perplexity(X_ts, index_ts)\n",
    "print(loss)\n",
    "print(perpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    elbo = mdl.elbo(X_ts, index_ts, kl_weight=0.)\n",
    "    perpl = mdl.perplexity(X_ts, elbo)\n",
    "    print(perpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverse_transform_fn = lambda x: pd.to_datetime(scaler.inverse_transform(x)[:, 0], format='%Y').strftime('%Y')\n",
    "\n",
    "reload(dctm)\n",
    "tops = dctm.print_topics(\n",
    "    mdl, index_points=np.unique(index_tr)[::10], vocabulary=vocabulary,\n",
    "    inverse_transform_fn=inverse_transform_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
