{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.compat import v1 as tf1\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import pandas as pd\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import correlated_topic_model as ctmd\n",
    "import dynamic_correlated_topic_model as dctm\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "from imp import reload\n",
    "import os\n",
    "from scipy import sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET MAY BE DOWNLOADED FROM\n",
    "# https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases/data#\n",
    "\n",
    "# this assumes that the dataset has been downloaded and extracted in Downloads/combined.json\n",
    "\n",
    "import datasets\n",
    "df, corpus, vocabulary = datasets.get_doj('~/Downloads/combined.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler([-1, 1])\n",
    "index_points = scaler.fit_transform(df.days[:, None])\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.expand_dims(corpus.todense().astype(np.float64), -2)\n",
    "\n",
    "(X_tr, X_ts, index_tr, index_ts, X_tr_sorted, X_ts_sorted,\n",
    " index_tr_sorted, index_ts_sorted\n",
    ") = datasets.train_test_split(X, index_points, return_sorted=True)\n",
    "\n",
    "inverse_transform_fn = lambda x: pd.to_datetime(\n",
    "    scaler.inverse_transform(x)[:, 0], format='%Y-%m')\n",
    "df_train = pd.DataFrame(X_tr_sorted[:, 0, :])\n",
    "df_train['days'] = inverse_transform_fn(index_tr_sorted)\n",
    "\n",
    "df_test = pd.DataFrame(X_ts_sorted[:, 0, :])\n",
    "df_test['days'] = inverse_transform_fn(index_ts_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape: tr: {}, ts: {}\".format(X_tr.shape, X_ts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dok_tr = sp.dok_matrix(X_tr_sorted[:, 0, :])\n",
    "# dok_ts = sp.dok_matrix(X_ts_sorted[:, 0, :])\n",
    "\n",
    "# name = 'doj'\n",
    "# save_pickle(dok_tr, '../data/{}_tr_doc.pkl'.format(name))\n",
    "# save_pickle(dok_ts, '../data/{}_ts_doc.pkl'.format(name))\n",
    "# save_pickle(vocabulary, '../data/{}_vocabulary.pkl'.format(name))\n",
    "\n",
    "# save_pickle(index_tr_sorted, '../data/{}_tr_index.pkl'.format(name))\n",
    "# save_pickle(index_ts_sorted, '../data/{}_ts_index.pkl'.format(name))\n",
    "\n",
    "# X_sorted = np.vstack((X_tr_sorted[:, 0, :], X_ts_sorted[:, 0, :]))\n",
    "# print_to_file_for_gdtm(\n",
    "#     df_train.append(df_test),\n",
    "#     vocabulary,\n",
    "#     sp.dok_matrix(X_sorted), filename='doj_all',\n",
    "#     patth='../data/'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_points = X_tr.shape[0]\n",
    "\n",
    "batch_size = 50\n",
    "dataset = tf.data.Dataset.zip(\n",
    "    tuple(map(tf.data.Dataset.from_tensor_slices,\n",
    "              (X_tr, index_tr))))\n",
    "dataset = dataset.shuffle(n_training_points, reshuffle_each_iteration=True)\n",
    "data_tr = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_index_points_beta = np.linspace(-1, 1, 10)[:, None]\n",
    "inducing_index_points_mu = np.linspace(-1, 1, 10)[:, None]\n",
    "inducing_index_points_ell = np.linspace(-1, 1, 10)[:, None]\n",
    "\n",
    "dtype = np.float64\n",
    "amplitude_beta = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_beta')\n",
    "length_scale_beta = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_beta')\n",
    "kernel_beta = tfk.MaternOneHalf(amplitude=amplitude_beta, length_scale=length_scale_beta)\n",
    "\n",
    "amplitude_mu = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name=\"amplitude_mu\")\n",
    "length_scale_mu = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name=\"length_scale_mu\")\n",
    "kernel_mu = tfk.ExponentiatedQuadratic(amplitude=amplitude_mu, length_scale=length_scale_mu)\n",
    "\n",
    "amplitude_ell = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_ell')\n",
    "length_scale_ell = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_ell')\n",
    "kernel_ell = tfk.ExponentiatedQuadratic(amplitude=amplitude_ell, length_scale=length_scale_ell)\n",
    "\n",
    "reload(ctmd)\n",
    "reload(dctm);\n",
    "\n",
    "mdl = dctm.DCTM(\n",
    "    n_topics=30, n_words=vocabulary.size,\n",
    "    kernel_beta=kernel_beta,\n",
    "    index_points_beta=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_beta=inducing_index_points_beta,\n",
    "    kernel_ell=kernel_ell,\n",
    "    kernel_mu=kernel_mu,\n",
    "    index_points_mu=np.unique(index_tr)[:, None],\n",
    "    index_points_ell=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_mu=inducing_index_points_mu,\n",
    "    inducing_index_points_ell=inducing_index_points_ell,\n",
    "    layer_sizes=(500, 300, 200),\n",
    "    jitter_beta=1e-6,\n",
    "    jitter_mu=1e-5, \n",
    "    jitter_ell=1e-6,\n",
    "    encoder_jitter=1e-8,dtype=dtype)\n",
    "\n",
    "n_iter = 2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "losses = []\n",
    "perplexities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_directory = \"../tmp/training_checkpoints-30-topics\"\n",
    "# checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(model=mdl)\n",
    "\n",
    "# status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "# mdl = checkpoint.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(n_iter), disable=False)\n",
    "with tf.device('gpu'):    \n",
    "    for epoch in pbar:\n",
    "        loss_value = 0\n",
    "        perplexity_value = 0\n",
    "\n",
    "        for x_batch, index_points_batch in data_tr:\n",
    "            loss, perpl = mdl.batch_optimize(\n",
    "                x_batch,\n",
    "                optimizer=optimizer,\n",
    "                observation_index_points=index_points_batch,\n",
    "                trainable_variables=None,\n",
    "                kl_weight=float(x_batch.shape[0]) / float(n_training_points))\n",
    "            loss = tf.reduce_mean(loss, 0)\n",
    "            loss_value += loss\n",
    "            perplexity_value += perpl\n",
    "        \n",
    "        pbar.set_description(\n",
    "        'loss {:.3e}, perpl {:.3e}'.format(loss_value, perplexity_value))\n",
    "#         if epoch % 50 == 0:\n",
    "#             checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        perplexities.append(perplexity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_test(self, X, index_points, batch_size):\n",
    "    ntot = X.shape[0]\n",
    "    dataset = tf.data.Dataset.zip(\n",
    "        tuple(map(tf.data.Dataset.from_tensor_slices, (X, index_points))))\n",
    "    data_ts = dataset.batch(batch_size)\n",
    "    \n",
    "    log_perplexity = []\n",
    "    for x_batch, index_points_batch in data_tr:\n",
    "        words_per_document = tf.reduce_sum(input_tensor=x_batch, axis=-1)\n",
    "        elbo = self.elbo(\n",
    "            x_batch, observation_index_points=index_points_batch,\n",
    "            kl_weight=0.)\n",
    "        log_perplexity.extend([x for x in (-elbo / words_per_document)])\n",
    "    perplexity = tf.exp(tf.reduce_mean(log_perplexity))\n",
    "    return perplexity\n",
    "\n",
    "with tf.device('gpu'):\n",
    "    perpl = perplexity_test(mdl, X_ts, index_ts, batch_size=100)\n",
    "    print(perpl)\n",
    "# 484.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverse_transform_fn = lambda x: pd.to_datetime(scaler.inverse_transform(x)[:, 0]).strftime('%Y-%m')\n",
    "\n",
    "reload(dctm)\n",
    "tops = dctm.print_topics(\n",
    "    mdl, index_points=np.unique(index_tr)[::10], vocabulary=vocabulary,\n",
    "    inverse_transform_fn=inverse_transform_fn, top_n_topic=5, top_n_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_points = np.linspace(-1,1, 100)[:,None]\n",
    "corr_sample, Sigma_sample = dctm.get_correlation(mdl.surrogate_posterior_ell.sample(1200, index_points=test_points))\n",
    "corr_10p = tfp.stats.percentile(corr_sample, 5, axis=0)\n",
    "corr = tfp.stats.percentile(corr_sample, 50, axis=0)\n",
    "corr_90p = tfp.stats.percentile(corr_sample, 95, axis=0)\n",
    "Sigma_10p = tfp.stats.percentile(Sigma_sample, 5, axis=0)\n",
    "Sigma = tfp.stats.percentile(Sigma_sample, 50, axis=0)\n",
    "Sigma_90p = tfp.stats.percentile(Sigma_sample, 95, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.n_topics = mdl.surrogate_posterior_beta.batch_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(beta, vocab, top_n=10):\n",
    "    # account for multiple times -> in this case returns\n",
    "    # the most common (unique) words across time\n",
    "    # beta is for a single topic\n",
    "    dd = tf.reshape(\n",
    "        tf.tile(tf.expand_dims(vocab, -1), [1, beta.shape[-1]]), [-1])\n",
    "    idx = tf.argsort(tf.reshape(beta, [-1]))[::-1].numpy()\n",
    "\n",
    "    dd = iter(dd.numpy()[idx])\n",
    "    top_words = []\n",
    "    while len(top_words) < top_n:\n",
    "        x = next(dd).decode('utf8')\n",
    "        if x not in top_words:\n",
    "            top_words.append(x)\n",
    "    return top_words\n",
    "\n",
    "    for topic_num in range(mdl.n_topics):\n",
    "        wt = words_topic[:, topic_num, :]\n",
    "        topics.append(' '.join(top_words(wt, vocabulary, top_n=top_n_topic)))\n",
    "        print('Topic {}: {}'.format(topic_num, topics[-1]))\n",
    "        for t, time in enumerate(times_display):\n",
    "            topics_t = (\n",
    "                top_words(wt[:, t, None], vocabulary, top_n=top_n_time))\n",
    "            print('- at t={}: {}'.format(time, ' '.join(topics_t)))\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = tops\n",
    "topic_num = 0\n",
    "\n",
    "plt.title(\"Topic {}: {}\".format(topic_num, topics[topic_num]))\n",
    "for t in range(mdl.n_topics)[:10]:\n",
    "    if t == topic_num:# or t not in [1,8]:\n",
    "        continue\n",
    "    plt.plot(corr[:, topic_num, t], label='{}:{}'.format(t, topics[t]))\n",
    "plt.xticks(range(test_points.size)[::10], inverse_transform_fn(test_points)[::10], rotation=45);\n",
    "# plt.legend();\n",
    "\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tr = mdl.predict(X_tr)[:,0,:].numpy()\n",
    "cc = np.zeros([mdl.n_topics, np.unique(index_tr).size])\n",
    "for j, i in enumerate(np.unique(index_tr)):\n",
    "    idx = (np.abs(index_tr-i)<1e-7).flatten()\n",
    "    cc[:, j] = topic_tr[idx].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.nn.softmax((mdl.surrogate_posterior_mu.get_marginal_distribution(test_points).mean()), axis=0)\n",
    "\n",
    "colors = plt.cm.jet(np.linspace(0, 1, mdl.n_topics))\n",
    "for i in range(30):\n",
    "    for t in range(i,i+1):\n",
    "        plt.plot(test_points, mu[t], label=topics[t], color=colors[i]);\n",
    "\n",
    "    for t in range(i,i+1):\n",
    "        plt.plot(np.unique(index_tr), cc[t], label='{}'.format(topics[t]), color=colors[t])\n",
    "\n",
    "    plt.xticks(test_points[::10], inverse_transform_fn(test_points)[::10], rotation=45);\n",
    "    plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
